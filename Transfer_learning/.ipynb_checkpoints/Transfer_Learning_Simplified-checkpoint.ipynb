{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33dbd497-0207-416c-b1f9-9a8e9711cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets,models,transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1d24c-a7d6-4a12-83f9-b9f71de95798",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91eff886-46e6-4929-b909-748b6846c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train' : transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ]),\n",
    "    'val' : transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da13912-6962-4a53-adb8-a0eb60f73e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names : ['ants', 'bees']\n",
      "There are 61 batches in the train set\n",
      "There are 39 batches in test set\n",
      "There are 244 training images\n",
      "There are 153 testing images\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'hymenoptera_data'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x]) for x in ['train','val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],batch_size = 4, shuffle = True) for x in ['train','val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "# print the results\n",
    "print(f\"Class Names : {class_names}\")\n",
    "print(f\"There are {len(dataloaders['train'])} batches in the train set\")\n",
    "print(f\"There are {len(dataloaders['val'])} batches in test set\")\n",
    "print(f\"There are {dataset_sizes['train']} training images\")\n",
    "print(f\"There are {dataset_sizes['val']} testing images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a511d83-e98d-45ac-8128-94f4de1c688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the resnet\n",
    "\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4281a974-bd41-490e-b615-9c4b67c1310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the layers in the network\n",
    "for param in model_conv.parameters():\n",
    "\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22917c03-8f50-43a6-b339-5203c916a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of imputs of the last layer(or number of neurons in the layer preceeding the last layer)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "# Reconstruct the last layer (output layer) to have only two classes\n",
    "model_conv.fc = nn.Linear(num_ftrs,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11dd11d8-7781-4a26-a81a-a0200a2f9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model_conv = model_conv.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "267bf889-a534-472e-9f6d-e880470451c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For one iteration, this is what happens:\n",
      "Input shape: torch.Size([4, 3, 224, 224])\n",
      "labels shape: torch.Size([4])\n",
      "labels are:tensor([0, 1, 1, 1], device='cuda:0')\n",
      "Output Tensor: tensor([[-0.4324,  0.1662],\n",
      "        [ 0.0979,  0.8776],\n",
      "        [ 0.1966,  0.1094],\n",
      "        [-0.0096,  0.2617]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Outputs shape torch.Size([4, 2])\n",
      "predicted: tensor([1, 1, 0, 1], device='cuda:0')\n",
      "predicted shape torch.Size([4])\n",
      "Correct predictions: tensor(2, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Understand what's happening\n",
    "iteration = 0\n",
    "correct = 0\n",
    "\n",
    "for inputs,labels in dataloaders['train']:\n",
    "    if iteration ==1:\n",
    "        break\n",
    "        \n",
    "    inputs = Variable(inputs)\n",
    "    labels = Variable(labels)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    print(\"For one iteration, this is what happens:\")\n",
    "    print(\"Input shape:\",inputs.shape)\n",
    "    print(\"labels shape:\",labels.shape)\n",
    "    print(\"labels are:{}\".format(labels))\n",
    "    # Forward propogation\n",
    "    output = model_conv(inputs)\n",
    "    print(\"Output Tensor:\",output)\n",
    "    print(\"Outputs shape\",output.shape)\n",
    "    _,predicted = torch.max(output,1)\n",
    "    print(\"predicted:\",predicted)\n",
    "    print(\"predicted shape\",predicted.shape)\n",
    "    correct += (predicted == labels).sum()\n",
    "    print(\"Correct predictions:\",correct)\n",
    "    \n",
    "    iteration +=1 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789326a7-1110-4c39-ac3b-873f1f028628",
   "metadata": {},
   "source": [
    "- In the above Input shape: torch.Size([4, 3, 224, 224])\n",
    "- 3 is the number of channels. 4 is the batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1466d402-c32d-41c3-bf97-c22d5ecd616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_conv.fc.parameters(),lr = 0.001, momentum=0.9)\n",
    "\n",
    "# Try experimenting with optim.Adam(model_conv.fc.parameters(),lr = 0.001)\n",
    "# Decay LR by as factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer,step_size=8,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea3328-7cc5-4e32-915f-54c2e69e2886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b277a2-eb1a-49bf-9b5f-c6efb06cdc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df666f63-1179-4664-a69b-8aae2d873fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe98e7-9d1e-477e-b9ae-b91d656c0702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60b571-6f4d-402e-a09b-a0d042792340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417484d-071b-402a-9192-5034ee3000ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce13a34-9379-4665-9b16-d8af89761a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecec5c4-e773-4590-b4ed-6d8f2a75e5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd50c54-8314-43ef-b921-0bfc117428a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab7e97-3020-4a9c-aeea-a5b9505e0341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350b836-e105-4117-b898-122b3d0846c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be1e31-5a76-4842-8c25-35e8469f9203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d642e-904c-404a-bf66-b549d8085991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c52d95-0f4c-4be3-85df-ff18aeaa8d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81891e78-8a43-4385-bcd5-c8bfa3c34538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8770d4f-4044-4acf-b86e-376bb5b3b2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1950ba-69f8-4f58-b611-88064bfe55bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c89e3-d9a8-43ca-8d3d-189efc9b7c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5f396-bbca-431d-9209-e50095ff987e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_gpu]",
   "language": "python",
   "name": "conda-env-torch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
