{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf12bed-d2e4-48b4-864e-d4956ea1750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d37cc-fda7-4234-8483-87a0c8b4d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_size,embedding,n_layers=1,dropout=0):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        # because our input size is a word embedding ith number of gfeatures == hodeen_size\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if\n",
    "                                                                   n_layers==1\n",
    "                                                                   else dropout),birectional=True)\n",
    "    def forward(self,input_seq,input_lengths,hidden=None):\n",
    "        # input_seq: batch of input_sentences,shape=max_length,batch_size)\n",
    "        # input_length = list of sentence lengths corresponding to each sentence in the batch\n",
    "        # hidden_state of shape : n_layersxnum_directions,batch_size,hidden_size\n",
    "        # convert word indexes to embeddings\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_packed_sequence(embedded,input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs,hidden = self.gru(packed,hidden)\n",
    "        # unpack padding\n",
    "        outputs,_ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum birectional GRU outputs\n",
    "        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs,hidden\n",
    "    \n",
    "        # outputs: the output features h_t from the last layer of the GRU, for each timestep(sum of bidirectional outputs)\n",
    "        # outputs shape = (max_lenght,batch_size,hidden_size)\n",
    "        # hidden: hidden state for the last timestep, of shape=(n_layers x num_deirections, batch_size,hidden_size)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac8fd2-6eeb-4825-8840-ae56eda64bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Luong attention layer\n",
    "class Attn(torch.nn.Modulr):\n",
    "    def __init__(self,method,hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self,hidden,encoder_output):\n",
    "        # Element wise Multiply the current target decoder state with the encoder output and sum them\n",
    "        return torch.sum(hidden*encoder_output,dim=2)\n",
    "    \n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "        # hidden of shape: (1,batch_size,hidden_size)\n",
    "        # encoder_outputs of shape: (max_length,batch_size,hidden_size)\n",
    "        # (1,batch_size,hidden_size) * (max_length,batch_size,hidden_size)= (max_lenght,batch_sioze,hidden_size)\n",
    "        \n",
    "        # calculate the attention weights (energies)\n",
    "        attn_energies = self.dot_score(hidden,encoder_outputs) # (max_len,batch_size)\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        att_energies = attn_energies.t() # (batch_size,max_length)\n",
    "        # Return the softmax normalized probability scores(with added dimensions)\n",
    "        return F.softmax(attn_energies,dim=1).unsqueeze(1) # (batch_size,1,max_length)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80548e-22d6-459a-82b5-25629a0179c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea89d9-2ca9-4f7a-a148-e40dde7e8768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d02186-07ba-4365-83cf-d3ec28d47388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b9e6f-deff-470e-94ab-a04cbfa089e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847794a-ef3f-4549-abac-3e7f20e9f9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22f793-c78f-4bf9-861f-fc28e9a57c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6ee2b-0266-4648-8136-578a9b50d45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba4a7b-6213-4ceb-a754-7fba59f2f258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f782438-4775-4eff-bff8-1b28c7197231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c705d3a-e750-4fae-a4b6-a2025680f00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0bfa3-6426-429d-b993-a6025baa5123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv] *",
   "language": "python",
   "name": "conda-env-torchenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
